{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amirmotefaker/predicting-sales-e-commerce?scriptVersionId=143874120\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Predicting Sales E-Commerce\n\n- Accurate sales forecasts enable retailers to more precisely plan for the future of an e-commerce business. Sales forecasting is an important part of meeting customer expectations and provides insight into how the market will react to any given product.\n\n- We live in the world of e-commerce. We see tons of different stores here and there through the web. Internet made it possible to trade with anyone and everywhere. We can buy goods without leaving our house, we can compare prices in different stores within seconds, we can find what we really want and do not accept just the first more or less suitable offer. And I believe it would be really interesting to look at this world through the data it produces. That's why I decided to play around with e-commerce numbers and try to understand it better.\n\n- The data used in this analysis is taken from Kaggle dataset \"E-Commerce Data | Actual transactions of UK retailer\".\n\n- This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\n- As always, we start our analysis by setting up our environment and by importing necessary libraries.\n\n- We import standard numpy and pandas to be able to perform analysis with Python, also we need data visualization libraries matplotlib and seaborn to output interesting visual findings.\n\n\nPredicting sales in e-commerce is a critical aspect of managing an online retail business. Accurate sales predictions can help you make informed decisions about inventory management, marketing strategies, and overall business growth. Here are some key methods and considerations for predicting sales in e-commerce:\n\n1. Historical Data Analysis:\n   - Start by analyzing your historical sales data. Look at trends, seasonality, and any patterns that may emerge. This data can provide valuable insights into past performance.\n\n2. Time Series Forecasting:\n   - Time series forecasting methods, such as ARIMA (AutoRegressive Integrated Moving Average) or Prophet, can be used to predict future sales based on past sales data. These models take into account seasonality and trends.\n\n3. Machine Learning Models:\n   - Machine learning models like regression, decision trees, random forests, and neural networks can be trained on historical data to make predictions. Feature engineering, which involves selecting and transforming relevant input features, is crucial for these models.\n\n4. Customer Segmentation:\n   - Segment your customer base to identify different groups with varying buying behaviors. This can help you tailor marketing efforts and predict sales more accurately for each segment.\n\n5. External Factors:\n   - Consider external factors that can influence sales, such as holidays, economic conditions, and industry trends. Incorporating these factors into your models can improve their accuracy.\n\n6. Inventory Management:\n   - Accurate sales predictions are essential for inventory management. Overstocking or understocking can lead to lost sales or excess costs. Make sure your predictions align with your inventory levels.\n\n7. Marketing Campaign Analysis:\n   - Analyze the impact of marketing campaigns on sales. Attribution modeling can help you understand which marketing efforts are driving the most sales, allowing you to optimize your marketing strategy.\n\n8. A/B Testing:\n   - Conduct A/B tests to experiment with different strategies and determine which ones have the most significant impact on sales. This can help refine your predictions over time.\n\n9. Customer Feedback and Reviews:\n   - Pay attention to customer feedback and product reviews. Negative feedback can lead to decreased sales, while positive feedback can boost sales. Monitor sentiment and address customer concerns promptly.\n\n10. Forecasting Tools:\n    - Consider using specialized forecasting software or e-commerce analytics platforms that offer pre-built models and dashboards tailored for sales predictions.\n\n11. Continuous Monitoring and Iteration:\n    - Sales prediction models should be regularly monitored and updated as new data becomes available. E-commerce is dynamic, and market conditions can change rapidly.\n\n12. Use of AI and Predictive Analytics:\n    - Advanced artificial intelligence and predictive analytics techniques, such as deep learning and reinforcement learning, can provide more accurate and automated sales predictions, especially for large datasets.\n\n- Remember that sales predictions are not always 100% accurate, but with consistent refinement and the use of various data sources and models, you can improve their accuracy over time. Additionally, be prepared to adapt your strategies based on the insights gained from your predictions to maximize your e-commerce success.\n\n\n\n- Online Retail Data Set: [DataSet](https://archive.ics.uci.edu/ml/datasets/online+retail#)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Import libraries and Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:10.552222Z","iopub.execute_input":"2023-09-21T11:02:10.552841Z","iopub.status.idle":"2023-09-21T11:02:11.592739Z","shell.execute_reply.started":"2023-09-21T11:02:10.552794Z","shell.execute_reply":"2023-09-21T11:02:11.591512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/onlineretail/OnlineRetail.csv\", encoding=\"latin\", dtype={'CustomerID': str})\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:11.597053Z","iopub.execute_input":"2023-09-21T11:02:11.597754Z","iopub.status.idle":"2023-09-21T11:02:12.667493Z","shell.execute_reply.started":"2023-09-21T11:02:11.597722Z","shell.execute_reply":"2023-09-21T11:02:12.666075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:12.669865Z","iopub.execute_input":"2023-09-21T11:02:12.670344Z","iopub.status.idle":"2023-09-21T11:02:12.691386Z","shell.execute_reply.started":"2023-09-21T11:02:12.670291Z","shell.execute_reply":"2023-09-21T11:02:12.690217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Just by looking at the first 5 rows of our table, we can understand the structure and datatypes present in our dataset.\n- We can notice that we will have to deal with time series data, integers and floats, and categorical, and text data.","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis(EDA)\n- Every data science project starts with EDA as we have to understand what do we have to deal with.\n- I divide EDA into 2 types: visual and numerical. Let's start with numerical as the simple pandas method .describe() gives us a lot of useful information.","metadata":{}},{"cell_type":"markdown","source":"## Quick statistical overview","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:12.694316Z","iopub.execute_input":"2023-09-21T11:02:12.69544Z","iopub.status.idle":"2023-09-21T11:02:12.750478Z","shell.execute_reply.started":"2023-09-21T11:02:12.695385Z","shell.execute_reply":"2023-09-21T11:02:12.749313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Just a quick look at data with the .describe() method gives us a lot of space to think.\n- We see negative quantities and prices, and we can see that not all records have CustomerID data.\n- We can also see that the majority of transactions are for quantities from 3 to 10 items, majority of items have prices up to 5 pounds.\n- We have a bunch of huge outliers we will have to deal with later.","metadata":{}},{"cell_type":"markdown","source":"## Dealing with types\n- .read_csv() method performs basic type check, but it doesn't do that perfectly.\n- That's why it is much better to deal with data types in our dataframe before any modifications to prevent additional difficulties.\n- Every pandas dataframe has an attribute .dtypes which will help us understand what we currently have and what data has to be casted to correct types.","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:12.752097Z","iopub.execute_input":"2023-09-21T11:02:12.752814Z","iopub.status.idle":"2023-09-21T11:02:12.762893Z","shell.execute_reply.started":"2023-09-21T11:02:12.752772Z","shell.execute_reply":"2023-09-21T11:02:12.761286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If we have DateTime data it's better to cast it to DateTime type.\n- We don't touch InvoiceNo for now as it seems like data in this column has not only numbers.\n- We saw just the first 5 rows, while pandas during import scanned all the data and found that the type here is not numerical.","metadata":{}},{"cell_type":"code","source":"df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\ndf = df.set_index('InvoiceDate')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:12.764502Z","iopub.execute_input":"2023-09-21T11:02:12.765049Z","iopub.status.idle":"2023-09-21T11:02:13.160044Z","shell.execute_reply.started":"2023-09-21T11:02:12.765006Z","shell.execute_reply":"2023-09-21T11:02:13.158997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:13.161676Z","iopub.execute_input":"2023-09-21T11:02:13.162404Z","iopub.status.idle":"2023-09-21T11:02:13.178563Z","shell.execute_reply.started":"2023-09-21T11:02:13.16236Z","shell.execute_reply":"2023-09-21T11:02:13.177386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dealing with null values\n- Next and very important step is dealing with missing values.\n\n- Normally if you encounter null values in the dataset you have to understand nature of those null values and possible impact they could have on the model.\n\n- There are few strategies that we can use to fix our issue with null values:\n\n    - delete rows with null values\n    - delete the feature with null values\n    - impute data with mean or median values or use another imputing strategy (method .fillna())","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:13.180069Z","iopub.execute_input":"2023-09-21T11:02:13.180427Z","iopub.status.idle":"2023-09-21T11:02:13.459337Z","shell.execute_reply.started":"2023-09-21T11:02:13.180397Z","shell.execute_reply":"2023-09-21T11:02:13.458028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- CustomerID has too many null values and this feature cannot predict a lot so we can just drop it.\n- It could be reasonable to create another feature \"Amount of orders per customer\".","metadata":{}},{"cell_type":"code","source":"df = df.drop(columns=['CustomerID'])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:13.46111Z","iopub.execute_input":"2023-09-21T11:02:13.461829Z","iopub.status.idle":"2023-09-21T11:02:13.491253Z","shell.execute_reply.started":"2023-09-21T11:02:13.461787Z","shell.execute_reply":"2023-09-21T11:02:13.490033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's check out what kind of nulls we have in Description:\n","metadata":{}},{"cell_type":"code","source":"df[df['Description'].isnull()].head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:13.497175Z","iopub.execute_input":"2023-09-21T11:02:13.497556Z","iopub.status.idle":"2023-09-21T11:02:13.570456Z","shell.execute_reply.started":"2023-09-21T11:02:13.497524Z","shell.execute_reply":"2023-09-21T11:02:13.569302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The data in these rows is pretty strange as UnitPrice is 0, so these orders do not generate any sales.\n- We can impute it with \"UNKNOWN ITEM\" at the moment and deal with those later during the analysis.","metadata":{}},{"cell_type":"code","source":"df['Description'] = df['Description'].fillna('UNKNOWN ITEM')\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:13.572087Z","iopub.execute_input":"2023-09-21T11:02:13.572446Z","iopub.status.idle":"2023-09-21T11:02:13.883663Z","shell.execute_reply.started":"2023-09-21T11:02:13.572414Z","shell.execute_reply":"2023-09-21T11:02:13.882513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking out columns separately\n- It makes sense to go feature by feature and check what pitfalls we have in our data and also to understand our numbers better.\n\n- Let's continue checking the Description column. Here we can see items that were bought most often.","metadata":{}},{"cell_type":"code","source":"df['Description'].value_counts().head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:13.885524Z","iopub.execute_input":"2023-09-21T11:02:13.885941Z","iopub.status.idle":"2023-09-21T11:02:13.999972Z","shell.execute_reply.started":"2023-09-21T11:02:13.885902Z","shell.execute_reply":"2023-09-21T11:02:13.998853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Here we can see our best-selling products, items that appear in orders the most often.","metadata":{}},{"cell_type":"markdown","source":"# To make it visually more appealing let's create a bar chart for 15 top items","metadata":{}},{"cell_type":"code","source":"# Which items were bought more often?\nitem_counts = df['Description'].value_counts().sort_values(ascending=False).iloc[0:15]\nplt.figure(figsize=(18,6))\nsns.barplot(x=item_counts.index, y=item_counts.values, palette=sns.cubehelix_palette(15))\nplt.ylabel(\"Counts\")\nplt.title(\"Which items were bought more often?\");\nplt.xticks(rotation=90);","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:14.001841Z","iopub.execute_input":"2023-09-21T11:02:14.002233Z","iopub.status.idle":"2023-09-21T11:02:14.851092Z","shell.execute_reply.started":"2023-09-21T11:02:14.002198Z","shell.execute_reply":"2023-09-21T11:02:14.84966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Description'].value_counts().tail()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:14.852457Z","iopub.execute_input":"2023-09-21T11:02:14.853385Z","iopub.status.idle":"2023-09-21T11:02:14.98241Z","shell.execute_reply.started":"2023-09-21T11:02:14.853332Z","shell.execute_reply":"2023-09-21T11:02:14.981219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We also notice from the above code that valid items are normally uppercase and non-valid or cancelations are in lowercase.","metadata":{}},{"cell_type":"code","source":"df[~df['Description'].str.isupper()]['Description'].value_counts().head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:14.984006Z","iopub.execute_input":"2023-09-21T11:02:14.984357Z","iopub.status.idle":"2023-09-21T11:02:15.330138Z","shell.execute_reply.started":"2023-09-21T11:02:14.984326Z","shell.execute_reply":"2023-09-21T11:02:15.328938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- A quick check of the case of letters in the Description says that there are some units with lowercase letters in their name and also that lowercase records are for canceled items.\n- Here we can understand that data management in the store can be improved.","metadata":{}},{"cell_type":"code","source":"# Not full upper case items\nlcase_counts = df[~df['Description'].str.isupper()]['Description'].value_counts().sort_values(ascending=False).iloc[0:15]\nplt.figure(figsize=(18,6))\nsns.barplot(x=lcase_counts.index, y=lcase_counts.values, palette=sns.color_palette(\"hls\", 15))\nplt.ylabel(\"Counts\")\nplt.title(\"Not full upper case items\");\nplt.xticks(rotation=90);","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:15.331394Z","iopub.execute_input":"2023-09-21T11:02:15.332683Z","iopub.status.idle":"2023-09-21T11:02:16.219369Z","shell.execute_reply.started":"2023-09-21T11:02:15.332618Z","shell.execute_reply":"2023-09-21T11:02:16.218039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Checking out stoke codes looks like they are deeply correlated with descriptions - which makes perfect sense.","metadata":{}},{"cell_type":"code","source":"df['StockCode'].value_counts().head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:16.221474Z","iopub.execute_input":"2023-09-21T11:02:16.221927Z","iopub.status.idle":"2023-09-21T11:02:16.331771Z","shell.execute_reply.started":"2023-09-21T11:02:16.221885Z","shell.execute_reply":"2023-09-21T11:02:16.330682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Which stock codes were used the most?\nstock_counts = df['StockCode'].value_counts().sort_values(ascending=False).iloc[0:15]\nplt.figure(figsize=(18,6))\nsns.barplot(x=stock_counts.index, y=stock_counts.values, palette=sns.color_palette(\"GnBu_d\"))\nplt.ylabel(\"Counts\")\nplt.title(\"Which stock codes were used the most?\");\nplt.xticks(rotation=90);","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:16.333787Z","iopub.execute_input":"2023-09-21T11:02:16.334267Z","iopub.status.idle":"2023-09-21T11:02:16.920282Z","shell.execute_reply.started":"2023-09-21T11:02:16.334214Z","shell.execute_reply":"2023-09-21T11:02:16.919024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking out also InvoiceNo feature","metadata":{}},{"cell_type":"code","source":"df['InvoiceNo'].value_counts().tail()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:16.921609Z","iopub.execute_input":"2023-09-21T11:02:16.922003Z","iopub.status.idle":"2023-09-21T11:02:17.020888Z","shell.execute_reply.started":"2023-09-21T11:02:16.921973Z","shell.execute_reply":"2023-09-21T11:02:17.019695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Which invoices had the most items?\ninv_counts = df['InvoiceNo'].value_counts().sort_values(ascending=False).iloc[0:15]\nplt.figure(figsize=(18,6))\nsns.barplot(x=inv_counts.index, y=inv_counts.values, palette=sns.color_palette(\"BuGn_d\"))\nplt.ylabel(\"Counts\")\nplt.title(\"Which invoices had the most items?\");\nplt.xticks(rotation=90);","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:17.022374Z","iopub.execute_input":"2023-09-21T11:02:17.022823Z","iopub.status.idle":"2023-09-21T11:02:17.593755Z","shell.execute_reply.started":"2023-09-21T11:02:17.022783Z","shell.execute_reply":"2023-09-21T11:02:17.592559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['InvoiceNo'].str.startswith('C')].describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:17.595557Z","iopub.execute_input":"2023-09-21T11:02:17.596284Z","iopub.status.idle":"2023-09-21T11:02:17.935994Z","shell.execute_reply.started":"2023-09-21T11:02:17.59624Z","shell.execute_reply":"2023-09-21T11:02:17.93519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Looks like Invoices that start with 'C' are the \"Canceling\"/\"Returning\" invoices. This resolves the mystery of negative quantities.\n\n- Although, we should've gotten deeper into the analysis of those returns, for the sake of simplicity let's just ignore those values for the moment.\n\n- We can actually start a separate project based on that data and predict the returning/canceling rates for the store.","metadata":{}},{"cell_type":"code","source":"df = df[~df['InvoiceNo'].str.startswith('C')]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:17.93723Z","iopub.execute_input":"2023-09-21T11:02:17.93774Z","iopub.status.idle":"2023-09-21T11:02:18.298764Z","shell.execute_reply.started":"2023-09-21T11:02:17.937711Z","shell.execute_reply":"2023-09-21T11:02:18.297739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:18.299889Z","iopub.execute_input":"2023-09-21T11:02:18.300849Z","iopub.status.idle":"2023-09-21T11:02:18.358344Z","shell.execute_reply.started":"2023-09-21T11:02:18.300804Z","shell.execute_reply":"2023-09-21T11:02:18.357196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- During exploratory data analysis we can go back to the same operations and checks, just to understand how our actions affected the dataset.\n\n- EDA is the series of repetitive tasks to understand better our data.\n\n- Here, for example we get back to .describe() method to get an overall picture of our data after some manipulations.\n\n- We still see negative quantities and negative prices, let's get into those records.","metadata":{}},{"cell_type":"code","source":"# df[df['Quantity'] < 0]\ndf[df['Quantity'] < 0].head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:18.359792Z","iopub.execute_input":"2023-09-21T11:02:18.360264Z","iopub.status.idle":"2023-09-21T11:02:18.379255Z","shell.execute_reply.started":"2023-09-21T11:02:18.360222Z","shell.execute_reply":"2023-09-21T11:02:18.378118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Here we can see that other \"Negative quantities\" appear to be damaged/lost/unknown items.\n- Again, we will just ignore them for the sake of simplicity of analysis for this project.","metadata":{}},{"cell_type":"code","source":"df = df[df['Quantity'] > 0]\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:18.381161Z","iopub.execute_input":"2023-09-21T11:02:18.381929Z","iopub.status.idle":"2023-09-21T11:02:18.490424Z","shell.execute_reply.started":"2023-09-21T11:02:18.381894Z","shell.execute_reply":"2023-09-21T11:02:18.489153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We also see negative UnitPrice, which is not normal as well.\n- Let's check this out:","metadata":{}},{"cell_type":"code","source":"df[df['UnitPrice'] < 0].describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:18.49206Z","iopub.execute_input":"2023-09-21T11:02:18.492708Z","iopub.status.idle":"2023-09-21T11:02:18.510818Z","shell.execute_reply.started":"2023-09-21T11:02:18.492674Z","shell.execute_reply":"2023-09-21T11:02:18.509422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['UnitPrice'] == -11062.06]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:18.512346Z","iopub.execute_input":"2023-09-21T11:02:18.512731Z","iopub.status.idle":"2023-09-21T11:02:18.528326Z","shell.execute_reply.started":"2023-09-21T11:02:18.512697Z","shell.execute_reply":"2023-09-21T11:02:18.526983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As there are just two rows, let's ignore them for the moment (the description gives us enough warnings, although we still need some context to understand it better)","metadata":{}},{"cell_type":"code","source":"df = df[df['UnitPrice'] > 0]\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:18.54037Z","iopub.execute_input":"2023-09-21T11:02:18.540799Z","iopub.status.idle":"2023-09-21T11:02:18.644873Z","shell.execute_reply.started":"2023-09-21T11:02:18.540765Z","shell.execute_reply":"2023-09-21T11:02:18.643696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As we have finished cleaning our data and removed all suspicious records we can start creating some new features for our model.\n- Let's start with the most obvious one - Sales.\n- We have quantities, we have prices - we can calculate the revenue.","metadata":{}},{"cell_type":"markdown","source":"# Visual EDA","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(3,6))\nsns.countplot(x=df[df['Country'] == 'United Kingdom']['Country'])\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:18.646521Z","iopub.execute_input":"2023-09-21T11:02:18.646912Z","iopub.status.idle":"2023-09-21T11:02:19.536662Z","shell.execute_reply.started":"2023-09-21T11:02:18.646879Z","shell.execute_reply":"2023-09-21T11:02:19.535432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18,6))\nsns.countplot(x=df[df['Country'] != 'United Kingdom']['Country'])\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:19.538181Z","iopub.execute_input":"2023-09-21T11:02:19.538654Z","iopub.status.idle":"2023-09-21T11:02:20.348192Z","shell.execute_reply.started":"2023-09-21T11:02:19.538596Z","shell.execute_reply":"2023-09-21T11:02:20.347047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uk_count = df[df['Country'] == 'United Kingdom']['Country'].count()\nall_count = df['Country'].count()\nuk_perc = uk_count/all_count\nprint(str('{0:.2f}%').format(uk_perc*100))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:20.350079Z","iopub.execute_input":"2023-09-21T11:02:20.350498Z","iopub.status.idle":"2023-09-21T11:02:20.609059Z","shell.execute_reply.started":"2023-09-21T11:02:20.350462Z","shell.execute_reply":"2023-09-21T11:02:20.608119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above plots and calculations, we can see that the vast majority of sales were made in the UK and just 8.49% went abroad.\n- We can say our dataset is skewed to the UK side.","metadata":{}},{"cell_type":"markdown","source":"# Detecting outliers\n- There are a few different methods to detect outliers:\n\n    - box plots,\n    - using IQR,\n    - scatter plot also works in some cases (and this is one of those).","metadata":{}},{"cell_type":"markdown","source":"### Detecting outliers using a scatter plot is pretty intuitive. You plot your data and remove data points that visually are definitely out of range. Like in the chart below:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,6))\nplt.scatter(x=df.index, y=df['UnitPrice'])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:20.610491Z","iopub.execute_input":"2023-09-21T11:02:20.611616Z","iopub.status.idle":"2023-09-21T11:02:23.029166Z","shell.execute_reply.started":"2023-09-21T11:02:20.611577Z","shell.execute_reply":"2023-09-21T11:02:23.028292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove obvious outliers:","metadata":{}},{"cell_type":"code","source":"df = df[df['UnitPrice'] < 25000]\nplt.figure(figsize=(18,6))\nplt.scatter(x=df.index, y=df['UnitPrice'])\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:23.030517Z","iopub.execute_input":"2023-09-21T11:02:23.03146Z","iopub.status.idle":"2023-09-21T11:02:25.735781Z","shell.execute_reply.started":"2023-09-21T11:02:23.031414Z","shell.execute_reply":"2023-09-21T11:02:25.734521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- After removing obvious outliers we still see some values that are out of normal distribution.","metadata":{}},{"cell_type":"markdown","source":"### To understand better the distribution of our data let's check out different percentiles of our numeric features:","metadata":{}},{"cell_type":"code","source":"df.quantile([0.05, 0.95, 0.98, 0.99, 0.999])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:25.737163Z","iopub.execute_input":"2023-09-21T11:02:25.737492Z","iopub.status.idle":"2023-09-21T11:02:31.567256Z","shell.execute_reply.started":"2023-09-21T11:02:25.737461Z","shell.execute_reply":"2023-09-21T11:02:31.565489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that if we remove the top 2% of our data points we will get rid of absolute outliers and will have a more balanced dataset.","metadata":{}},{"cell_type":"code","source":"df_quantile = df[df['UnitPrice'] < 125]\nplt.scatter(x=df_quantile.index, y=df_quantile['UnitPrice'])\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.56876Z","iopub.status.idle":"2023-09-21T11:02:31.569184Z","shell.execute_reply.started":"2023-09-21T11:02:31.568983Z","shell.execute_reply":"2023-09-21T11:02:31.569004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_quantile.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.570266Z","iopub.status.idle":"2023-09-21T11:02:31.570718Z","shell.execute_reply.started":"2023-09-21T11:02:31.570489Z","shell.execute_reply":"2023-09-21T11:02:31.570508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Looks like our data is almost ready for modelling.\n- We performed a clean up, we removed outliers that were disturbing the balance of our dataset, we removed invalid records.\n- Now our data looks much better! and it doesn't lose it's value.","metadata":{}},{"cell_type":"markdown","source":"# Visually checking distribution of numeric features","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsns.distplot(df_quantile[df_quantile['UnitPrice'] < 10]['UnitPrice'].values, kde=True, bins=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.572297Z","iopub.status.idle":"2023-09-21T11:02:31.572867Z","shell.execute_reply.started":"2023-09-21T11:02:31.572575Z","shell.execute_reply":"2023-09-21T11:02:31.572601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsns.distplot(df_quantile[df_quantile['UnitPrice'] < 5]['UnitPrice'].values, kde=True, bins=10, color='green')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.574246Z","iopub.status.idle":"2023-09-21T11:02:31.574812Z","shell.execute_reply.started":"2023-09-21T11:02:31.57451Z","shell.execute_reply":"2023-09-21T11:02:31.574535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From these histograms, we can see that the vast majority of items sold in this store have a low price range - 0 to 3 pounds.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsns.distplot(df_quantile[df_quantile['Quantity'] <= 30]['Quantity'], kde=True, bins=10, color='red')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.576904Z","iopub.status.idle":"2023-09-21T11:02:31.577317Z","shell.execute_reply.started":"2023-09-21T11:02:31.577123Z","shell.execute_reply":"2023-09-21T11:02:31.577142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsns.distplot(df_quantile[df_quantile['Quantity'] <= 15]['Quantity'], kde=True, bins=10, color='orange')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.579583Z","iopub.status.idle":"2023-09-21T11:02:31.580168Z","shell.execute_reply.started":"2023-09-21T11:02:31.579863Z","shell.execute_reply":"2023-09-21T11:02:31.579888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From these histograms we that people bought normally 1-5 items or 10-12\n- Maybe there was some kind of offers for sets?","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsns.distplot(df_quantile[df_quantile['UnitPrice'] < 60]['UnitPrice'], kde=True, bins=10, color='purple')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.583259Z","iopub.status.idle":"2023-09-21T11:02:31.584115Z","shell.execute_reply.started":"2023-09-21T11:02:31.583815Z","shell.execute_reply":"2023-09-21T11:02:31.583853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsns.distplot(df_quantile[df_quantile['UnitPrice'] < 30]['UnitPrice'], kde=True, bins=10, color='grey')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.585616Z","iopub.status.idle":"2023-09-21T11:02:31.586469Z","shell.execute_reply.started":"2023-09-21T11:02:31.586181Z","shell.execute_reply":"2023-09-21T11:02:31.58621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From these histograms, we can understand that majority of sales per order were in the range 1-15 pounds each.","metadata":{}},{"cell_type":"markdown","source":"# Analysing sales over time","metadata":{}},{"cell_type":"code","source":"df_ts = df[['UnitPrice']]\ndf_ts.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.58798Z","iopub.status.idle":"2023-09-21T11:02:31.588798Z","shell.execute_reply.started":"2023-09-21T11:02:31.588497Z","shell.execute_reply":"2023-09-21T11:02:31.588524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As we can see every invoice has its own timestamp (definitely based on the time the order was made).\n- We can resample time data by, for example, weeks, and try to see if there are any patterns in our sales.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,6))\ndf_resample = df_ts.resample('W').sum()\ndf_resample.plot()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.590284Z","iopub.status.idle":"2023-09-21T11:02:31.591095Z","shell.execute_reply.started":"2023-09-21T11:02:31.590804Z","shell.execute_reply":"2023-09-21T11:02:31.59084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- That week with 0 sales in January looks suspicious, let's check it closer.","metadata":{}},{"cell_type":"code","source":"df_resample['12-2010':'01-2011']","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.592548Z","iopub.status.idle":"2023-09-21T11:02:31.593586Z","shell.execute_reply.started":"2023-09-21T11:02:31.593277Z","shell.execute_reply":"2023-09-21T11:02:31.593306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Now it makes sense - possibly, during the New Year holidays period the store was closed and didn't process orders, that's why they didn't make any sales.","metadata":{}},{"cell_type":"markdown","source":"# Preparing data for modeling and feature creation\n- Now comes the most fun part of the project - building a model.\n- To do this we will need to create a few more additional features to make our model more sophisticated.","metadata":{}},{"cell_type":"code","source":"df_clean = df[df['UnitPrice'] < 15]\ndf_clean.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.595194Z","iopub.status.idle":"2023-09-21T11:02:31.596063Z","shell.execute_reply.started":"2023-09-21T11:02:31.595759Z","shell.execute_reply":"2023-09-21T11:02:31.595786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean.index","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.597771Z","iopub.status.idle":"2023-09-21T11:02:31.598313Z","shell.execute_reply.started":"2023-09-21T11:02:31.598038Z","shell.execute_reply":"2023-09-21T11:02:31.598064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quantity per invoice feature\n- A feature that could influence the sales output could be \"Quantity per invoice\". Let's find the data for this feature.","metadata":{}},{"cell_type":"code","source":"df_join = df_clean.groupby('InvoiceNo')[['Quantity']].sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.600055Z","iopub.status.idle":"2023-09-21T11:02:31.600889Z","shell.execute_reply.started":"2023-09-21T11:02:31.600569Z","shell.execute_reply":"2023-09-21T11:02:31.600598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_join = df_join.reset_index()\ndf_join.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.602295Z","iopub.status.idle":"2023-09-21T11:02:31.603157Z","shell.execute_reply.started":"2023-09-21T11:02:31.602862Z","shell.execute_reply":"2023-09-21T11:02:31.602889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean['InvoiceDate'] = df_clean.index\ndf_clean = df_clean.merge(df_join, how='left', on='InvoiceNo')\ndf_clean = df_clean.rename(columns={'Quantity_x' : 'Quantity', 'Quantity_y' : 'QuantityInv'})\ndf_clean.tail(15)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.604996Z","iopub.status.idle":"2023-09-21T11:02:31.605511Z","shell.execute_reply.started":"2023-09-21T11:02:31.605249Z","shell.execute_reply":"2023-09-21T11:02:31.605275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.606891Z","iopub.status.idle":"2023-09-21T11:02:31.607844Z","shell.execute_reply.started":"2023-09-21T11:02:31.607543Z","shell.execute_reply":"2023-09-21T11:02:31.60757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.609195Z","iopub.status.idle":"2023-09-21T11:02:31.609741Z","shell.execute_reply.started":"2023-09-21T11:02:31.609452Z","shell.execute_reply":"2023-09-21T11:02:31.609477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.611279Z","iopub.status.idle":"2023-09-21T11:02:31.611813Z","shell.execute_reply.started":"2023-09-21T11:02:31.611527Z","shell.execute_reply":"2023-09-21T11:02:31.611553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bucketing Quantity and UnitPrice features\n- Based on the EDA done previously we can group these features into 6 buckets for Quantity and 5 for UnitePrice using the pandas.cut() method.","metadata":{}},{"cell_type":"code","source":"bins_q = pd.IntervalIndex.from_tuples([(0, 2), (2, 5), (5, 8), (8, 11), (11, 14), (15, 5000)])\ndf_clean['QuantityRange'] = pd.cut(df_clean['Quantity'], bins=bins_q)\nbins_p = pd.IntervalIndex.from_tuples([(0, 1), (1, 2), (2, 3), (3, 4), (4, 20)])\ndf_clean['PriceRange'] = pd.cut(df_clean['UnitPrice'], bins=bins_p)\ndf_clean.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.613105Z","iopub.status.idle":"2023-09-21T11:02:31.613668Z","shell.execute_reply.started":"2023-09-21T11:02:31.613363Z","shell.execute_reply":"2023-09-21T11:02:31.61339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting and bucketing dates\n- We have noticed that depending on the season gifts sell differently:\n\n    - pick of sales is in the Q4\n    - then it drastically drops in Q1 of the next year\n    - and continues to grow till its new pick in Q4 again.\n\n\n- From this observation, we can create another feature that could improve our model.","metadata":{}},{"cell_type":"code","source":"df_clean['Month'] = df_clean['InvoiceDate'].dt.month\ndf_clean.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.615765Z","iopub.status.idle":"2023-09-21T11:02:31.616295Z","shell.execute_reply.started":"2023-09-21T11:02:31.616024Z","shell.execute_reply":"2023-09-21T11:02:31.616049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bins_d = pd.IntervalIndex.from_tuples([(0,3),(3,6),(6,9),(9,12)])\ndf_clean['DateRange'] = pd.cut(df_clean['Month'], bins=bins_d, labels=['q1','q2','q3','q4'])\ndf_clean.tail()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.617623Z","iopub.status.idle":"2023-09-21T11:02:31.618204Z","shell.execute_reply.started":"2023-09-21T11:02:31.617896Z","shell.execute_reply":"2023-09-21T11:02:31.61793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a model","metadata":{}},{"cell_type":"markdown","source":"## Splitting data into UK and non-UK\n- We have to analyze these 2 datasets separately to have more standardized data for a model because there can be some patterns that work for other countries and do not for the UK or vice versa.\n- Also a hypothesis to test - does the model built for the UK performs well on data for other countries?","metadata":{}},{"cell_type":"code","source":"df_uk = df_clean[df_clean['Country'] == 'United Kingdom']\ndf_abroad = df_clean[df_clean['Country'] != 'United Kingdom']","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.619386Z","iopub.status.idle":"2023-09-21T11:02:31.619926Z","shell.execute_reply.started":"2023-09-21T11:02:31.619649Z","shell.execute_reply":"2023-09-21T11:02:31.619675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_uk.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.621716Z","iopub.status.idle":"2023-09-21T11:02:31.62398Z","shell.execute_reply.started":"2023-09-21T11:02:31.623673Z","shell.execute_reply":"2023-09-21T11:02:31.623702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting features and creating dummy variables","metadata":{}},{"cell_type":"code","source":"df_uk_model = df_uk[['UnitPrice', 'QuantityInv', 'QuantityRange', 'PriceRange', 'DateRange']]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.625286Z","iopub.status.idle":"2023-09-21T11:02:31.625821Z","shell.execute_reply.started":"2023-09-21T11:02:31.625532Z","shell.execute_reply":"2023-09-21T11:02:31.625556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_uk_model.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.627542Z","iopub.status.idle":"2023-09-21T11:02:31.628087Z","shell.execute_reply.started":"2023-09-21T11:02:31.627812Z","shell.execute_reply":"2023-09-21T11:02:31.627836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data = df_uk_model.copy()\ndf_data = pd.get_dummies(df_data, columns=['QuantityRange'], prefix='qr')\ndf_data = pd.get_dummies(df_data, columns=['PriceRange'], prefix='pr')\ndf_data = pd.get_dummies(df_data, columns=['DateRange'], prefix='dr')\ndf_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.629699Z","iopub.status.idle":"2023-09-21T11:02:31.630231Z","shell.execute_reply.started":"2023-09-21T11:02:31.629958Z","shell.execute_reply":"2023-09-21T11:02:31.629984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling\n- As the majority of our features are in the 0-1 range it would make sense to scale the \"QuantityInv\" feature too.\n- In general, scaling features is normally a good idea.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import scale\n\ndf_data['QuantityInv'] = scale(df_data['QuantityInv'])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.631676Z","iopub.status.idle":"2023-09-21T11:02:31.632185Z","shell.execute_reply.started":"2023-09-21T11:02:31.631927Z","shell.execute_reply":"2023-09-21T11:02:31.631952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train-Test Split\n- Now we have to split our data into train-test data to be able to train our model and validate its capabilities.","metadata":{}},{"cell_type":"code","source":"y = df_data['UnitPrice']\nX = df_data.drop(columns=['UnitPrice'])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.633683Z","iopub.status.idle":"2023-09-21T11:02:31.634206Z","shell.execute_reply.started":"2023-09-21T11:02:31.633947Z","shell.execute_reply":"2023-09-21T11:02:31.633971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.635549Z","iopub.status.idle":"2023-09-21T11:02:31.636086Z","shell.execute_reply.started":"2023-09-21T11:02:31.635814Z","shell.execute_reply":"2023-09-21T11:02:31.635838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing and validating different models\n- We use GridSearch and CrossValidation to test three types of regressors:\n    - Linear\n    - Decision Tree\n    - RandomForest","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.637542Z","iopub.status.idle":"2023-09-21T11:02:31.639068Z","shell.execute_reply.started":"2023-09-21T11:02:31.63877Z","shell.execute_reply":"2023-09-21T11:02:31.638798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"code","source":"fit_intercepts = [True, False]\nparam_grid_linear = dict(fit_intercept=fit_intercepts)\nlinear_model = LinearRegression()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.64045Z","iopub.status.idle":"2023-09-21T11:02:31.641001Z","shell.execute_reply.started":"2023-09-21T11:02:31.640723Z","shell.execute_reply":"2023-09-21T11:02:31.64075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"min_tree_splits = range(2,3)\nmin_tree_leaves = range(2,3)\nparam_grid_tree = dict(min_samples_split=min_tree_splits,\n                       min_samples_leaf=min_tree_leaves)\ntree_model = DecisionTreeRegressor()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.64268Z","iopub.status.idle":"2023-09-21T11:02:31.643454Z","shell.execute_reply.started":"2023-09-21T11:02:31.643005Z","shell.execute_reply":"2023-09-21T11:02:31.643069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"estimators_space = [100]\nmin_sample_splits = range(2,4)\nmin_sample_leaves = range(2,3)\nparam_grid_forest = dict(min_samples_split=min_sample_splits,\n                       min_samples_leaf=min_sample_leaves,\n                       n_estimators=estimators_space)\nforest_model = RandomForestRegressor()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.645038Z","iopub.status.idle":"2023-09-21T11:02:31.645425Z","shell.execute_reply.started":"2023-09-21T11:02:31.645239Z","shell.execute_reply":"2023-09-21T11:02:31.645257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing and validating","metadata":{}},{"cell_type":"code","source":"cv = 5\n\nmodels_to_test = ['LinearRegression','DecisionTreeRegressor','RandomForest']\nregression_dict = dict(LinearRegression=linear_model,\n                       DecisionTreeRegressor=tree_model,\n                       RandomForest=forest_model)\nparam_grid_dict = dict(LinearRegression=param_grid_linear,\n                       DecisionTreeRegressor=param_grid_tree,\n                       RandomForest=param_grid_forest)\n\nscore_dict = {}\nparams_dict = {}\nmae_dict = {}\nmse_dict = {}\nr2_dict = {}\nbest_est_dict = {}\n\nfor model in models_to_test:\n  regressor = GridSearchCV(regression_dict[model], param_grid_dict[model], cv=cv, n_jobs=-1)\n\n  regressor.fit(X_train, y_train)\n  y_pred = regressor.predict(X_test)\n\n  # Print the tuned parameters and score\n  print(\" === Start report for regressor {} ===\".format(model))\n  score_dict[model] = regressor.best_score_\n  print(\"Tuned Parameters: {}\".format(regressor.best_params_)) \n  params_dict = regressor.best_params_\n  print(\"Best score is {}\".format(regressor.best_score_))\n\n  # Compute metrics\n  mae_dict[model] = mean_absolute_error(y_test, y_pred)\n  print(\"MAE for {}\".format(model))\n  print(mean_absolute_error(y_test, y_pred))\n  mse_dict[model] = mean_squared_error(y_test, y_pred)\n  print(\"MSE for {}\".format(model))\n  print(mean_squared_error(y_test, y_pred))\n  r2_dict[model] = r2_score(y_test, y_pred)\n  print(\"R2 score for {}\".format(model))\n  print(r2_score(y_test, y_pred))\n  print(\" === End of report for regressor {} === \\n\".format(model))\n  \n  # Add best estimator to the dict\n  best_est_dict[model] = regressor.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.647271Z","iopub.status.idle":"2023-09-21T11:02:31.647696Z","shell.execute_reply.started":"2023-09-21T11:02:31.64747Z","shell.execute_reply":"2023-09-21T11:02:31.647488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating summary report","metadata":{}},{"cell_type":"code","source":"summary_cols = ['Best Score']\nsummary = pd.DataFrame.from_dict(r2_dict, orient='index')\nsummary.index.name = 'Regressor'\nsummary.columns = summary_cols\nsummary = summary.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.648944Z","iopub.status.idle":"2023-09-21T11:02:31.649328Z","shell.execute_reply.started":"2023-09-21T11:02:31.649142Z","shell.execute_reply":"2023-09-21T11:02:31.64916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing results","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nplt.xlabel('Best score')\nplt.title('Regressor Comparison')\n\nsns.barplot(x='Best Score', y='Regressor', data=summary)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T11:02:31.653373Z","iopub.status.idle":"2023-09-21T11:02:31.653882Z","shell.execute_reply.started":"2023-09-21T11:02:31.653644Z","shell.execute_reply":"2023-09-21T11:02:31.653675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n- This is a basic analysis of a transactions dataset with a model that predicts sales.\n\n- Random Forest Regressor appears to be the best model for our prediction with an R2 score of more than 0.6 which is not that bad.","metadata":{}}]}